<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Hanqi Yan</title>

    <meta name="author" content="Jon Barron">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Hanqi Yan
                </p>
                <p>I'm a PostDoc at <a href="https://kcl.ac.uk/">King's college London</a>, supervised by <a href="https://sites.google.com/view/yulanhe">Prof. Yulan He</a>, where I am focusing on robust and reliable language models. 
                </p>
                <p>
                I passed my PhD viva with no corrections after a great time in <a href = https://warwick.ac.uk/"> University of Warwick</a> (2020.10-2024.04), advised by Prof. <a href="https://sites.google.com/view/yulanhe">Prof. Yulan He</a> and Dr. <a href="https://sites.google.com/view/lin-gui/about-me">Lin Gui</a>. I finished my M.S. at <a href = https://english.pku.edu.cn/">Peking University</a> (2017-2020) and my B.E. at <a href="https://ev.buaa.edu.cn/">Beihang University</a> (2013-2017).
                </p>
<p>During Ph.D., I started my Causality Journey in visiting professor <a href="https://www.andrew.cmu.edu/user/kunz1/"> Kun Zhang </a> affiliated with Causal Learning and Reasoning Group@CMU. Before Ph.D., I started my NLP journey in visiting professor <a href ="https://www4.comp.polyu.edu.hk/~cswjli/"> Wenjie Li</a> affiliated Natural Language Processing Group @PolyU Hong Kong.</p>
                <p style="text-align:center">
                  <a href="mailto:hanqi.yan@kcl.ac.uk">Email</a> &nbsp;/&nbsp;
                  <a href="https://github.com/hanqi-qi/homepage/blob/main/HanqiYanCV_202404.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=YmWi1lgAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://twitter.com/yan_hanqi">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://github.com/hanqi-qi/">Github</a>
                </p>
                <p> Strive not to be a success, but rather to be of value. -- Albert Einstein </p>
              </td>

              <td style="padding:2.5%;width:30%;max-width:30%">
                <a href="images/self_2024.png"><img style="width:80%;max-width:80%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/self_2024.png" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research Summary</h2>
                <p>
                 My research interests lie in the intersection of Machine Learning and Natural Language Processing, i.e., incorporating fundamental representation learning to enhance the <b>interpretability</b> and <b>robustness</b> of different NLP models.</p>
<ol>
<li> Founded by Representation Learning: I address the intrinsic limitations in representations learnt in Transformers structure: order sensitivity <a href="https://arxiv.org/abs/2402.15637"> [InfoAC] </a>, dimension collapse <a href="https://openreview.net/forum?id=BtUxE_8i5l5"> [TokenUni] </a>]]; explore the principled disentanglement <a href="https://neurips.cc/virtual/2023/paper_metadata_from_author/71063">[Matte]</a>; study the neuron-level interpretability in human-preference alignment <a href="https://arxiv.org/abs/2406.17969v1">[DecPO]</a>.</li> 
<li> <b>Extend the Impact to Practical NLP Applications</b> </li>: 
<ul>
  <li>Text classification <a href="https://aclanthology.org/D19-1554/">[LexicalAT]</a>,<a href="https://aclanthology.org/2022.cl-4.17/">[HINT]</a>,<a href="https://aclanthology.org/2023.findings-eacl.102/">[DiscAda])</a></li>
  <li>Recommendation System <a href="https://dl.acm.org/doi/abs/10.1145/3397271.3401237">[GCQN]</a>, <a href="https://arxiv.org/abs/2305.05331">[GIANT]</a></li>
  <li>(Causal) Relation Extraction <a href="https://arxiv.org/abs/2406.18245">([ReWire]</a>,<a href="https://arxiv.org/abs/2106.03518">[KAG]</a></li> 
    <li>controllable generation <a href="https://neurips.cc/virtual/2023/paper_metadata_from_author/71063">[MATTE]</a> and Reasoning <a href="https://arxiv.org/abs/2402.14963v1">[Mirror]</a>.</li>
</ul>
</ol>
                
    </td>
    </tr>
  </tbody></table>
  
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:35%;vertical-align:middle">
                <h2> Publication</h2>
              </td>
            </tr>

              <tr>
              <td style="padding:20px;width:35%;vertical-align:middle">
                <h3> Large Language Model</h3>
              </td>
              </tr> 
            
                <tr>
              <td style="padding:20px;width:35%;vertical-align:middle">
                 <img src="images/mirror.png" alt="icl" width="350" height="150"> 
              </td>
              <td width="99%" valign="middle"> 
                 <a href="https://arxiv.org/pdf/2402.14963.pdf"> 
                  <span class="papertitle"> Mirror: A Multiple-perspective Self-Reflection Method for Knowledge-rich Reasoning</span>
                </a>
                <br>
                <strong>H. Yan</strong>, Q. Zhu, X. Wang, L. Gui, Y. He
                <br>
                <em>ACL24</em>
                <p>Introduce a Navigator model to interact with the Reasoner by providing question-specific and diverse guidance in knowledge-rich self-reflection process without any supervision.</p>
              </td>
            </tr>

              <tr>
              <td style="padding:20px;width:35%;vertical-align:middle">
                 <img src="images/infoAC.png" alt="icl" width="350" height="130"> 
              </td>
              <td width="99%" valign="middle"> 
                 <a href="https://arxiv.org/pdf/2402.15637"> 
                  <span class="papertitle">Addressing Order Sensitivity of In-Context Demonstration Examples in Causal Language Models. </span>
                </a>
                <br>
                Y. Xiang, <strong>H. Yan</strong>, L. Gui, Y. He
                <br>
                <em>ACL-findings</em>
                <p>We attribute the order sensitivity of CausalLMs to the auto-regressive attention masks, which restrict each token from accessing information from subsequent tokens. Thereby leading to our proposed consistencey-based representation learning method in addressing this vulnerability of LLMs. </p>
              </td>
            </tr>
      
            
            
              <tr>
              <td style="padding:10px;width:35%;vertical-align:middle">
                <img src="images/survey_macro.png" alt="icl" width="380" height="150">
              </td>
              <td width="99%" valign="middle"> 
                <a href="https://arxiv.org/abs/2311.00237">
                  <span class="papertitle">The Mystery and Fascination of LLMs: A Comprehensive Survey on the Interpretation and Analysis of Emergent Abilities.</span>
                </a>
                <br>
                Y. Zhou, J. Li, Y.Xiang, <strong>H.Yan</strong>, L. Gui, Y. He
                <br>
                <em>Under Review</em>
                <p>From Macro perspective, Why In-Context Learning can learn Different Algorithms without gradient descent, e.g, Regression, Bayesian.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:35%;vertical-align:middle">
                <img src="images/matte_casestudy.png" alt="matte" width="260" height="100">
              </td>
              <td width="99%" valign="middle">
                <a href="https://neurips.cc/virtual/2023/poster/71063">
                  <span class="papertitle">Counterfactual Generation with Identifiability Guarantee</span>
                </a>
                <br>
                <strong>H. Yan</strong>, L. Kong, L. Gui, Y. Chi, Eric. Xing, Y. He, K. Zhang.
                <br>
                <em>Neurips23</em>, 2023.
                <p> We observed the pitfalls of LLMs in detecting and intervening the implicit sentiment, so we provide Identification guarantees for successful disentanglement of the content and style variables. This principled representations can shed light on the llm alignments, i.e., safe and moral generation.</p>
              </td>
            </tr>
            
              <tr>
              <td style="padding:20px;width:35%;vertical-align:middle">
                <h3> Self-Explainable Models</h3>
              </td>
              </tr>
            
              <tr>
              <td style="padding:5px;width:35%;vertical-align:middle">
                <img src="images/giant.png" alt="giant" width="395" height="130">
              </td>
              <td width="99%" valign="middle">
                <a href="https://arxiv.org/abs/2305.05331">
                  <span class="papertitle">Explainable Recommender with Geometric Information Bottleneck</span>
                </a>
                <br>
                <strong>H. Yan</strong>, L. Gui, M. Wang, K. Zhang and Y. He
                <br>
                <em>TKDE</em>, 2023
                <p>To ease the humman annotation for rationales in Recommender, a prior from user-item interactions is incorporated into the textual latent factors for explaination generation.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:35%;vertical-align:middle">
                <img src="images/hint.png" alt="hint" width="350" height="220">
              </td>
              <td width="99%" valign="middle">
                <a href="https://direct.mit.edu/coli/article/doi/10.1162/coli_a_00459/112768/Hierarchical-Interpretation-of-Neural-Text">
                  <span class="papertitle">Hierarchical Interpretation of Neural Text Classification</span>
                </a>
                <br>
                <strong>H. Yan</strong>, L. Gui, M. Wang, K. Zhang and Y. He
                <br>
                <em>Computational Linguistics</em>, 2022, Presented at EMNLP22.
                <p>Unsupervised self-explanatory framework for document classification. It can extract word-, sentence-, and topic-level rationales explaining the document-level decision.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:35%;vertical-align:middle">
                <h3> Robustenss </h3>
              </td>
              </tr> 
            
            <tr>
              <td style="padding:10px;width:35%;vertical-align:middle">
                <img src="images/kag_new.png" alt="kag" width="350" height="180">
              </td>
              <td width="99%" valign="middle">
                <a href="https://aclanthology.org/2021.acl-long.261.pdf">
                  <span class="papertitle">A Knowledge-Aware Graph Model for Emotion Cause Extraction</span>
                </a>
                <br>
                <strong>H. Yan</strong>, L. Gui, G. Pergola and Y. He
                <br>
                <em>ACL</em>, 2021, <strong>Oral</strong>.
                <p>Commonsense Knowledge, i.e., ConceptNet is applied as invariant feature to tackle the distribution shift and Position Bias.</p>
              </td>
            </tr>

              <tr>
              <td style="padding:20px;width:35%;vertical-align:middle">
                <img src="images/matte_new.png" alt="matte" width="260" height="100">
              </td>
              <td width="99%" valign="middle">
                <a href="https://neurips.cc/virtual/2023/poster/71063">
                  <span class="papertitle">Counterfactual Generation with Identifiability Guarantee</span>
                </a>
                <br>
                <strong>H. Yan</strong>, L. Kong, L. Gui, Y. Chi, Eric. Xing, Y. He, K. Zhang.
                <br>
                <em>Neurips</em>, 2023.
                <p>Provide Identification guarantees for successful disentanglement of the content and style variables, further supports the intervention of latent attributes of the text.  This principled representations can shed light on the constrained, i.e., safe and moral generation for large language models with noisy pertaining data.</p>
              </td>
            </tr>

              <tr>
              <td style="padding:10px;width:35%;vertical-align:middle">
                <img src="images/softdecay.png" alt="matte" width="360" height="130">
              </td>
              <td width="99%" valign="middle">
                <a href="https://proceedings.mlr.press/v180/yan22b.html">
                  <span class="papertitle">Addressing Token Uniformity in Transformers via Singular Value Transformation</span>
                </a>
                <br>
                <strong>H. Yan</strong>, Gui, Y. Y. He.
                <br>
                <em>UAI</em>, 2022, <stong>Spotlight</stong>
                <p>Token uniformity implies  more vanished dimensions in the embedding space. _SoftDecay_ is proposed to a range of transformer-based language models and improved performance is observed in STS evaluation and a range of GLUE tasks.</p>
              </td>
            </tr>

                    <tr>
              <td style="padding:10px;width:35%;vertical-align:middle">
                <img src="images/incontext_adaptor.png" alt="matte" width="380" height="155">
              </td>
              <td width="99%" valign="middle">
                <a href="https://arxiv.org/abs/2302.06198">
                  <span class="papertitle">Distinguishability Calibration to In-Context Learning</span>
                </a>
                <br>
                H. Li, <strong>H. Yan</strong>, Y. Li, L. Qian, Y. He and L. Gui.
                <br>
                <em>EACL</em>, 2023
                <p>Token uniformity issue is still observed in in-context learning, we proposed an adaptor for more discriminative representation learning and improved performance is observed in fine-grained text classification tasks.</p>
              </td>
            </tr>
      

          </tbody></table>

          
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Professional Activities</h2>
              </td>
            </tr>
          </tbody></table>

          <table width="100%" align="center" border="0" cellpadding="20"><tbody>            
            <tr>
              <td width="75%" valign="center">
                <p> Event Organiser: Co-Chair of AACL-IJCNLP (Student Research Workshop) 2022 </p>
                <p> Reviewers for NLP: , AACL24,EACL23', EMNLP22',23'24, ACL23'24', NAACL24', </p> 
                <p> Reviewers for ML and AI: UAI23', AISTATS24', NEURIPS24', Neurocomputing, Knowledge and Information System, TOIS</p>
              </td>
            </tr> 
            </tbody></table>

          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <tr>
              <td align="center" style="padding:20px;width:25%;vertical-align:middle">
                <h2><br>Invited Talks</h2>
              </td>
              <td width="85%" valign="middle">
                <p> <strong>UC San Diego, NLP Group</strong>, 02/2024. Robust and Interpretable NLP via representation learning and Path Ahead </p>
                <p> <strong>Yale University, NLP Group</strong> 01/2024. Robust and Interpretable NLP via representation learning and Path Ahead </p>
                <p> <strong>Turing AI Fellowship Event</strong>, London, 03/2023, Distinguishability Calibration to In-Context Learning </p>
                <p> <strong>UKRI Fellows Workshop</strong>, University of Edinburgh, 04/2022. Interpreting Long Documents and Recommendation Systems via Latent Variable Models </p>

              </td>
            </tr>
                       

           <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <tr>
              <td align="center" style="padding:20px;width:25%;vertical-align:middle">
                <h2>Basically <br> Blog Posts</h2>
              </td>
              <td width="75%" valign="middle">
                <a href="https://github.com/hanqi-qi/Large_language_modeling/blob/main/Reading_Material.md">Reading List For Large Language Model</a>
                <br>
                <a href="https://zhuanlan.zhihu.com/p/652269984">Induction Head_ contribute to In-context Learning</a>
                <br>
                <a href="https://github.com/hanqi-qi/NLPReadingGroup/blob/main/CausalInference/CausalInference_Intro_hanqi.pdf">Causality101</a>
                <br>
                <a href="https://github.com/hanqi-qi/NLPReadingGroup/blob/main/CausalInference/CausalInference_RS_hanqi.pdf">Debised Recommendation with Causality</a>
                <br>
                <a href="https://zhuanlan.zhihu.com/p/665841340">Identifiability101 in Causality</a>
              </td>
            </tr>
            
            
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Feel free to steal this website's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>. <strong>Do not</strong> scrape the HTML from this page itself, as it includes analytics tags that you do not want on your own website &mdash; use the github code instead. Also, consider using <a href="https://leonidk.com/">Leonid Keselman</a>'s <a href="https://github.com/leonidk/new_website">Jekyll fork</a> of this page.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
